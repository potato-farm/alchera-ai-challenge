{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\tfrom torchsummary import summary as summary_\n",
    "except:\n",
    "\t!pip3 install torchsummary\n",
    "\tfrom torchsummary import summary as summary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib import import_module\n",
    "# DIRNAME = \"my\"\n",
    "# model = getattr(import_module(f\"{DIRNAME}.settings.model\"), \"getModel\")()\n",
    "\n",
    "# ----------- or\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model = smp.DeepLabV3Plus(\n",
    "\t\t\tencoder_name=\"efficientnet-b4\",\n",
    "\t\t\tencoder_weights=\"imagenet\",\n",
    "\t\t\tin_channels=3,\n",
    "\t\t\tclasses=15\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 31.75 GiB total capacity; 13.28 GiB already allocated; 5.75 MiB free; 13.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_79884/83636161.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m '''\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msummary_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mINPUTSIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/alchera/lib/python3.8/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/alchera/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/alchera/lib/python3.8/site-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/alchera/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/alchera/lib/python3.8/site-packages/segmentation_models_pytorch/encoders/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# Identity and Sequential stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# Block stages need drop_connect rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/alchera/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/alchera/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/alchera/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/alchera/lib/python3.8/site-packages/efficientnet_pytorch/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 31.75 GiB total capacity; 13.28 GiB already allocated; 5.75 MiB free; 13.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "INPUTSIZE = (3,1024,1024) # c h w\n",
    "BATCH = 8\n",
    "\n",
    "'''\n",
    "Params size == model.state_dict size!! (100mb 넘기면 안댐)\n",
    "Forward/backward pass size : GPU 메모리 할당량(대충 200000 넘어가면 좀 위험한듯? 확실친않음) \n",
    "'''\n",
    "\n",
    "summary_(model.to(\"cuda\"),INPUTSIZE,batch_size=BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[ 1.9723e-01,  1.7056e-02, -1.6311e-01,  ..., -1.3884e-01,\n           -2.7111e-01, -4.0337e-01],\n          [ 1.0943e-01, -3.2768e-02, -1.7497e-01,  ..., -2.1511e-01,\n           -2.9859e-01, -3.8207e-01],\n          [ 2.1639e-02, -8.2592e-02, -1.8682e-01,  ..., -2.9139e-01,\n           -3.2608e-01, -3.6077e-01],\n          ...,\n          [ 7.5222e-02,  1.3102e-02, -4.9019e-02,  ...,  2.5974e-01,\n            1.9908e-01,  1.3842e-01],\n          [ 1.9987e-02, -4.8750e-02, -1.1749e-01,  ...,  3.2819e-01,\n            2.9308e-01,  2.5797e-01],\n          [-3.5248e-02, -1.1060e-01, -1.8596e-01,  ...,  3.9664e-01,\n            3.8708e-01,  3.7752e-01]],\n\n         [[-4.1178e-01, -5.7137e-01, -7.3096e-01,  ..., -3.5284e-01,\n           -2.5729e-01, -1.6174e-01],\n          [-3.5610e-01, -4.8799e-01, -6.1988e-01,  ..., -3.8107e-01,\n           -3.1091e-01, -2.4074e-01],\n          [-3.0043e-01, -4.0462e-01, -5.0880e-01,  ..., -4.0930e-01,\n           -3.6452e-01, -3.1974e-01],\n          ...,\n          [-1.6546e-01, -1.3818e-01, -1.1090e-01,  ...,  4.8661e-03,\n           -4.3949e-02, -9.2764e-02],\n          [-9.8785e-02, -9.2075e-02, -8.5365e-02,  ...,  1.0306e-01,\n            6.0611e-02,  1.8159e-02],\n          [-3.2110e-02, -4.5968e-02, -5.9826e-02,  ...,  2.0126e-01,\n            1.6517e-01,  1.2908e-01]],\n\n         [[ 2.2542e-02, -8.1540e-02, -1.8562e-01,  ...,  3.9965e-01,\n            4.4568e-01,  4.9171e-01],\n          [-5.0108e-02, -1.2511e-01, -2.0011e-01,  ...,  4.2739e-01,\n            4.7441e-01,  5.2143e-01],\n          [-1.2276e-01, -1.6868e-01, -2.1461e-01,  ...,  4.5512e-01,\n            5.0313e-01,  5.5115e-01],\n          ...,\n          [ 3.8834e-01,  3.7297e-01,  3.5759e-01,  ...,  4.5938e-01,\n            4.0562e-01,  3.5185e-01],\n          [ 3.0258e-01,  2.8517e-01,  2.6776e-01,  ...,  4.2393e-01,\n            3.3991e-01,  2.5589e-01],\n          [ 2.1682e-01,  1.9737e-01,  1.7792e-01,  ...,  3.8848e-01,\n            2.7420e-01,  1.5993e-01]],\n\n         ...,\n\n         [[ 3.4741e-01,  2.3584e-01,  1.2427e-01,  ...,  1.7544e-01,\n            2.4686e-01,  3.1828e-01],\n          [ 2.1287e-01,  1.7017e-01,  1.2747e-01,  ...,  1.2000e-01,\n            1.1755e-01,  1.1510e-01],\n          [ 7.8341e-02,  1.0451e-01,  1.3068e-01,  ...,  6.4553e-02,\n           -1.1765e-02, -8.8084e-02],\n          ...,\n          [-4.4875e-01, -3.6838e-01, -2.8802e-01,  ..., -2.3969e-02,\n           -5.8804e-02, -9.3639e-02],\n          [-4.9142e-01, -4.0438e-01, -3.1734e-01,  ..., -3.2500e-02,\n           -8.0054e-02, -1.2761e-01],\n          [-5.3408e-01, -4.4037e-01, -3.4666e-01,  ..., -4.1032e-02,\n           -1.0130e-01, -1.6158e-01]],\n\n         [[-2.2963e-01, -9.5034e-02,  3.9564e-02,  ...,  7.7224e-01,\n            6.9977e-01,  6.2730e-01],\n          [-2.3090e-01, -1.2546e-01, -2.0028e-02,  ...,  7.3877e-01,\n            6.5512e-01,  5.7147e-01],\n          [-2.3217e-01, -1.5589e-01, -7.9619e-02,  ...,  7.0531e-01,\n            6.1047e-01,  5.1564e-01],\n          ...,\n          [-7.6478e-02, -3.4372e-02,  7.7338e-03,  ...,  1.5446e-01,\n            7.6978e-02, -4.9916e-04],\n          [-9.0981e-02, -5.4763e-02, -1.8545e-02,  ...,  1.6938e-01,\n            1.2236e-01,  7.5351e-02],\n          [-1.0548e-01, -7.5154e-02, -4.4824e-02,  ...,  1.8430e-01,\n            1.6775e-01,  1.5120e-01]],\n\n         [[ 2.7457e-01,  4.3686e-01,  5.9916e-01,  ...,  4.7177e-01,\n            3.2700e-01,  1.8223e-01],\n          [ 2.0599e-01,  3.0340e-01,  4.0082e-01,  ...,  2.3596e-01,\n            1.6958e-01,  1.0319e-01],\n          [ 1.3742e-01,  1.6995e-01,  2.0248e-01,  ...,  1.5490e-04,\n            1.2149e-02,  2.4144e-02],\n          ...,\n          [ 2.4798e-01,  2.0013e-01,  1.5229e-01,  ...,  1.2703e-01,\n            2.5708e-02, -7.5618e-02],\n          [ 2.0478e-01,  1.5089e-01,  9.6995e-02,  ...,  1.1871e-01,\n            4.1568e-02, -3.5578e-02],\n          [ 1.6159e-01,  1.0164e-01,  4.1701e-02,  ...,  1.1039e-01,\n            5.7427e-02,  4.4611e-03]]],\n\n\n        [[[ 2.4986e-01,  1.0168e-01, -4.6501e-02,  ...,  1.0877e-01,\n           -4.4508e-02, -1.9779e-01],\n          [ 1.4733e-01,  6.6593e-02, -1.4146e-02,  ...,  1.1724e-01,\n           -4.6214e-02, -2.0967e-01],\n          [ 4.4800e-02,  3.1505e-02,  1.8210e-02,  ...,  1.2572e-01,\n           -4.7921e-02, -2.2156e-01],\n          ...,\n          [ 3.3594e-01,  2.7814e-01,  2.2034e-01,  ..., -7.4903e-02,\n           -1.9095e-01, -3.0699e-01],\n          [ 3.1011e-01,  2.7669e-01,  2.4327e-01,  ...,  8.8705e-02,\n           -9.5630e-02, -2.7996e-01],\n          [ 2.8428e-01,  2.7524e-01,  2.6620e-01,  ...,  2.5231e-01,\n           -3.1185e-04, -2.5294e-01]],\n\n         [[-2.7626e-02, -2.4794e-01, -4.6825e-01,  ..., -2.9128e-01,\n           -4.6114e-01, -6.3100e-01],\n          [-8.0928e-02, -2.5230e-01, -4.2368e-01,  ..., -2.8062e-01,\n           -4.3731e-01, -5.9400e-01],\n          [-1.3423e-01, -2.5667e-01, -3.7910e-01,  ..., -2.6996e-01,\n           -4.1347e-01, -5.5699e-01],\n          ...,\n          [ 7.6389e-02, -2.1372e-02, -1.1913e-01,  ...,  5.7690e-02,\n           -1.6356e-01, -3.8481e-01],\n          [ 9.5482e-02, -3.8118e-02, -1.7172e-01,  ..., -8.6719e-02,\n           -3.1648e-01, -5.4623e-01],\n          [ 1.1458e-01, -5.4865e-02, -2.2431e-01,  ..., -2.3113e-01,\n           -4.6939e-01, -7.0765e-01]],\n\n         [[ 2.8413e-01,  2.0530e-01,  1.2648e-01,  ...,  2.4801e-01,\n            1.1024e-01, -2.7535e-02],\n          [ 3.9748e-01,  3.0064e-01,  2.0379e-01,  ...,  1.6022e-01,\n            3.1974e-02, -9.6277e-02],\n          [ 5.1084e-01,  3.9597e-01,  2.8110e-01,  ...,  7.2438e-02,\n           -4.6290e-02, -1.6502e-01],\n          ...,\n          [ 3.5863e-01,  3.6407e-01,  3.6952e-01,  ...,  1.4768e-01,\n            1.5900e-01,  1.7032e-01],\n          [ 2.8860e-01,  3.0362e-01,  3.1864e-01,  ...,  1.2206e-01,\n            1.6177e-01,  2.0148e-01],\n          [ 2.1858e-01,  2.4317e-01,  2.6776e-01,  ...,  9.6444e-02,\n            1.6454e-01,  2.3264e-01]],\n\n         ...,\n\n         [[-9.5423e-02, -7.8814e-02, -6.2205e-02,  ..., -2.7797e-01,\n           -4.1013e-01, -5.4230e-01],\n          [-1.8416e-01, -1.1012e-01, -3.6089e-02,  ..., -2.0018e-01,\n           -3.6600e-01, -5.3181e-01],\n          [-2.7289e-01, -1.4143e-01, -9.9727e-03,  ..., -1.2240e-01,\n           -3.2186e-01, -5.2132e-01],\n          ...,\n          [-6.4727e-02, -3.0328e-02,  4.0714e-03,  ..., -2.0391e-01,\n           -2.3118e-01, -2.5844e-01],\n          [-3.7827e-01, -3.0605e-01, -2.3382e-01,  ..., -2.5482e-01,\n           -2.6660e-01, -2.7838e-01],\n          [-6.9181e-01, -5.8176e-01, -4.7172e-01,  ..., -3.0573e-01,\n           -3.0202e-01, -2.9831e-01]],\n\n         [[-5.0758e-01, -3.0870e-01, -1.0982e-01,  ..., -4.1521e-02,\n            3.4631e-03,  4.8447e-02],\n          [-4.6122e-01, -2.9115e-01, -1.2107e-01,  ..., -1.0316e-01,\n           -3.3177e-02,  3.6806e-02],\n          [-4.1486e-01, -2.7359e-01, -1.3233e-01,  ..., -1.6480e-01,\n           -6.9817e-02,  2.5165e-02],\n          ...,\n          [ 4.9964e-02,  1.8247e-01,  3.1498e-01,  ...,  4.2523e-01,\n            4.5640e-01,  4.8757e-01],\n          [ 1.4015e-01,  1.8946e-01,  2.3878e-01,  ...,  5.1836e-01,\n            5.2130e-01,  5.2425e-01],\n          [ 2.3034e-01,  1.9646e-01,  1.6257e-01,  ...,  6.1148e-01,\n            5.8621e-01,  5.6093e-01]],\n\n         [[-8.4395e-02,  6.4099e-02,  2.1259e-01,  ...,  4.6179e-01,\n            3.0984e-01,  1.5789e-01],\n          [-1.3732e-02,  1.0981e-01,  2.3336e-01,  ...,  4.4725e-01,\n            3.5594e-01,  2.6464e-01],\n          [ 5.6932e-02,  1.5553e-01,  2.5412e-01,  ...,  4.3271e-01,\n            4.0205e-01,  3.7139e-01],\n          ...,\n          [ 3.0137e-01,  2.0462e-01,  1.0787e-01,  ..., -3.2754e-01,\n           -2.4809e-01, -1.6863e-01],\n          [ 2.9584e-01,  2.0643e-01,  1.1702e-01,  ..., -3.0227e-01,\n           -2.3579e-01, -1.6930e-01],\n          [ 2.9030e-01,  2.0824e-01,  1.2617e-01,  ..., -2.7700e-01,\n           -2.2349e-01, -1.6997e-01]]],\n\n\n        [[[ 5.6144e-01,  3.9428e-01,  2.2712e-01,  ..., -6.8626e-01,\n           -6.8149e-01, -6.7671e-01],\n          [ 4.8616e-01,  3.3062e-01,  1.7507e-01,  ..., -4.8428e-01,\n           -5.4620e-01, -6.0811e-01],\n          [ 4.1089e-01,  2.6696e-01,  1.2303e-01,  ..., -2.8229e-01,\n           -4.1090e-01, -5.3951e-01],\n          ...,\n          [ 5.8314e-01,  5.8616e-01,  5.8918e-01,  ...,  7.6213e-02,\n            1.5913e-02, -4.4387e-02],\n          [ 5.6922e-01,  6.0351e-01,  6.3780e-01,  ...,  6.3063e-02,\n            7.9089e-03, -4.7245e-02],\n          [ 5.5530e-01,  6.2086e-01,  6.8642e-01,  ...,  4.9913e-02,\n           -9.4743e-05, -5.0102e-02]],\n\n         [[-1.4186e-01, -3.9785e-01, -6.5385e-01,  ..., -3.2884e-01,\n           -5.2331e-01, -7.1778e-01],\n          [-2.3130e-02, -2.5397e-01, -4.8481e-01,  ..., -1.6826e-01,\n           -4.1561e-01, -6.6296e-01],\n          [ 9.5597e-02, -1.1009e-01, -3.1577e-01,  ..., -7.6717e-03,\n           -3.0791e-01, -6.0814e-01],\n          ...,\n          [-1.6003e-01,  1.5448e-02,  1.9093e-01,  ..., -1.6728e-01,\n           -2.5639e-01, -3.4551e-01],\n          [-2.3344e-01, -3.2168e-02,  1.6910e-01,  ..., -1.3236e-01,\n           -1.9766e-01, -2.6295e-01],\n          [-3.0685e-01, -7.9783e-02,  1.4728e-01,  ..., -9.7454e-02,\n           -1.3892e-01, -1.8039e-01]],\n\n         [[ 2.2652e-01,  2.6482e-01,  3.0312e-01,  ...,  5.8772e-01,\n            5.4553e-01,  5.0334e-01],\n          [ 2.2177e-01,  2.6775e-01,  3.1373e-01,  ...,  5.8257e-01,\n            5.6797e-01,  5.5336e-01],\n          [ 2.1703e-01,  2.7069e-01,  3.2435e-01,  ...,  5.7743e-01,\n            5.9040e-01,  6.0338e-01],\n          ...,\n          [ 6.8503e-01,  6.1926e-01,  5.5350e-01,  ...,  1.2259e-01,\n            1.6552e-01,  2.0845e-01],\n          [ 7.4823e-01,  6.5353e-01,  5.5883e-01,  ...,  2.4641e-01,\n            2.5794e-01,  2.6946e-01],\n          [ 8.1144e-01,  6.8780e-01,  5.6415e-01,  ...,  3.7023e-01,\n            3.5035e-01,  3.3047e-01]],\n\n         ...,\n\n         [[ 7.3668e-02,  1.3576e-01,  1.9785e-01,  ...,  1.1398e-01,\n            9.4916e-02,  7.5855e-02],\n          [ 5.7075e-02,  1.7968e-01,  3.0228e-01,  ...,  3.2863e-01,\n            2.8499e-01,  2.4136e-01],\n          [ 4.0483e-02,  2.2359e-01,  4.0670e-01,  ...,  5.4328e-01,\n            4.7507e-01,  4.0686e-01],\n          ...,\n          [-2.7637e-01, -2.1850e-01, -1.6063e-01,  ..., -2.7238e-01,\n           -3.9783e-01, -5.2329e-01],\n          [-3.1137e-01, -2.7769e-01, -2.4401e-01,  ..., -2.1242e-01,\n           -2.8609e-01, -3.5977e-01],\n          [-3.4637e-01, -3.3688e-01, -3.2740e-01,  ..., -1.5245e-01,\n           -1.7435e-01, -1.9625e-01]],\n\n         [[-1.3806e-01, -6.3174e-02,  1.1713e-02,  ...,  7.6715e-01,\n            6.2266e-01,  4.7817e-01],\n          [-1.1573e-01, -3.8754e-02,  3.8219e-02,  ...,  7.0534e-01,\n            5.3011e-01,  3.5488e-01],\n          [-9.3394e-02, -1.4334e-02,  6.4725e-02,  ...,  6.4354e-01,\n            4.3756e-01,  2.3158e-01],\n          ...,\n          [ 6.1683e-02,  1.7879e-01,  2.9590e-01,  ...,  2.8080e-01,\n            1.1641e-01, -4.7989e-02],\n          [ 9.3031e-02,  1.7694e-01,  2.6086e-01,  ...,  2.5769e-01,\n            1.2699e-01, -3.7045e-03],\n          [ 1.2438e-01,  1.7510e-01,  2.2581e-01,  ...,  2.3457e-01,\n            1.3757e-01,  4.0580e-02]],\n\n         [[-2.6942e-01, -1.1966e-01,  3.0093e-02,  ...,  1.7306e+00,\n            1.6062e+00,  1.4818e+00],\n          [-1.9402e-01, -7.5535e-02,  4.2946e-02,  ...,  1.4026e+00,\n            1.3235e+00,  1.2443e+00],\n          [-1.1861e-01, -3.1404e-02,  5.5800e-02,  ...,  1.0747e+00,\n            1.0407e+00,  1.0068e+00],\n          ...,\n          [ 3.0384e-01,  2.0259e-01,  1.0134e-01,  ..., -1.3548e-01,\n           -1.8867e-01, -2.4185e-01],\n          [ 2.2955e-01,  1.4316e-01,  5.6766e-02,  ..., -1.0927e-01,\n           -1.4329e-01, -1.7730e-01],\n          [ 1.5526e-01,  8.3722e-02,  1.2187e-02,  ..., -8.3058e-02,\n           -9.7905e-02, -1.1275e-01]]],\n\n\n        ...,\n\n\n        [[[ 7.2620e-01,  6.5993e-01,  5.9367e-01,  ..., -1.7719e-01,\n           -2.3733e-01, -2.9748e-01],\n          [ 7.5065e-01,  6.3870e-01,  5.2675e-01,  ..., -3.3605e-01,\n           -3.9058e-01, -4.4512e-01],\n          [ 7.7511e-01,  6.1747e-01,  4.5983e-01,  ..., -4.9491e-01,\n           -5.4383e-01, -5.9275e-01],\n          ...,\n          [ 7.2013e-01,  6.8708e-01,  6.5404e-01,  ...,  1.4350e-01,\n            3.5712e-02, -7.2073e-02],\n          [ 6.0518e-01,  6.1486e-01,  6.2454e-01,  ...,  1.7365e-01,\n            9.2190e-02,  1.0729e-02],\n          [ 4.9024e-01,  5.4263e-01,  5.9503e-01,  ...,  2.0381e-01,\n            1.4867e-01,  9.3530e-02]],\n\n         [[ 2.6387e-01,  2.2903e-02, -2.1807e-01,  ..., -5.1884e-01,\n           -7.5481e-01, -9.9079e-01],\n          [ 2.1018e-01, -5.3535e-03, -2.2088e-01,  ..., -6.1079e-01,\n           -8.1005e-01, -1.0093e+00],\n          [ 1.5648e-01, -3.3610e-02, -2.2370e-01,  ..., -7.0275e-01,\n           -8.6529e-01, -1.0278e+00],\n          ...,\n          [ 4.0235e-01,  3.2915e-01,  2.5596e-01,  ...,  9.0336e-02,\n           -4.5913e-02, -1.8216e-01],\n          [ 3.9449e-01,  3.4206e-01,  2.8963e-01,  ...,  3.3314e-02,\n           -8.0270e-02, -1.9385e-01],\n          [ 3.8662e-01,  3.5496e-01,  3.2330e-01,  ..., -2.3709e-02,\n           -1.1463e-01, -2.0555e-01]],\n\n         [[-3.1456e-01, -4.2233e-01, -5.3009e-01,  ...,  4.0599e-01,\n            3.0380e-01,  2.0160e-01],\n          [-1.9079e-01, -2.6839e-01, -3.4598e-01,  ...,  3.9555e-01,\n            3.4795e-01,  3.0035e-01],\n          [-6.7018e-02, -1.1445e-01, -1.6187e-01,  ...,  3.8511e-01,\n            3.9211e-01,  3.9910e-01],\n          ...,\n          [ 2.4621e-01,  1.8483e-01,  1.2344e-01,  ..., -4.4564e-02,\n           -2.3419e-02, -2.2730e-03],\n          [ 2.2376e-01,  1.8505e-01,  1.4634e-01,  ..., -1.4081e-01,\n           -1.0154e-01, -6.2276e-02],\n          [ 2.0130e-01,  1.8527e-01,  1.6923e-01,  ..., -2.3705e-01,\n           -1.7966e-01, -1.2228e-01]],\n\n         ...,\n\n         [[ 1.9614e-01,  3.9487e-02, -1.1716e-01,  ...,  1.8794e-01,\n           -6.7125e-02, -3.2219e-01],\n          [-2.3376e-02, -5.5405e-02, -8.7434e-02,  ...,  2.5266e-01,\n            3.4023e-02, -1.8462e-01],\n          [-2.4289e-01, -1.5030e-01, -5.7707e-02,  ...,  3.1738e-01,\n            1.3517e-01, -4.7039e-02],\n          ...,\n          [-3.1510e-01, -2.4687e-01, -1.7864e-01,  ...,  7.1667e-03,\n           -7.2840e-02, -1.5285e-01],\n          [-4.2355e-01, -3.8610e-01, -3.4866e-01,  ..., -1.0267e-01,\n           -1.7275e-01, -2.4284e-01],\n          [-5.3199e-01, -5.2533e-01, -5.1867e-01,  ..., -2.1251e-01,\n           -2.7267e-01, -3.3283e-01]],\n\n         [[-1.1507e-01,  3.0972e-02,  1.7701e-01,  ...,  6.3824e-01,\n            4.7635e-01,  3.1445e-01],\n          [-9.9742e-02,  5.6686e-02,  2.1312e-01,  ...,  5.6833e-01,\n            4.4036e-01,  3.1240e-01],\n          [-8.4415e-02,  8.2401e-02,  2.4922e-01,  ...,  4.9842e-01,\n            4.0438e-01,  3.1034e-01],\n          ...,\n          [-1.2691e-01, -8.8274e-02, -4.9642e-02,  ...,  2.8869e-01,\n            2.6378e-01,  2.3886e-01],\n          [-8.2452e-02, -8.4942e-02, -8.7432e-02,  ...,  1.8682e-01,\n            1.4778e-01,  1.0875e-01],\n          [-3.7998e-02, -8.1610e-02, -1.2522e-01,  ...,  8.4952e-02,\n            3.1793e-02, -2.1365e-02]],\n\n         [[ 1.0903e+00,  1.1994e+00,  1.3085e+00,  ...,  1.1025e+00,\n            1.0207e+00,  9.3888e-01],\n          [ 9.2782e-01,  9.8652e-01,  1.0452e+00,  ...,  8.3486e-01,\n            7.8991e-01,  7.4496e-01],\n          [ 7.6533e-01,  7.7362e-01,  7.8191e-01,  ...,  5.6720e-01,\n            5.5911e-01,  5.5103e-01],\n          ...,\n          [-8.0924e-02, -1.2340e-01, -1.6588e-01,  ..., -1.7224e-02,\n           -5.5708e-02, -9.4192e-02],\n          [-1.0880e-01, -1.8669e-01, -2.6458e-01,  ..., -2.1908e-02,\n           -2.7715e-02, -3.3523e-02],\n          [-1.3667e-01, -2.4998e-01, -3.6328e-01,  ..., -2.6591e-02,\n            2.7785e-04,  2.7147e-02]]],\n\n\n        [[[ 6.9271e-02,  5.7379e-02,  4.5486e-02,  ..., -2.6542e-01,\n           -2.6202e-01, -2.5862e-01],\n          [ 1.1233e-01,  1.1122e-01,  1.1012e-01,  ..., -3.5901e-01,\n           -3.3448e-01, -3.0994e-01],\n          [ 1.5538e-01,  1.6506e-01,  1.7475e-01,  ..., -4.5260e-01,\n           -4.0694e-01, -3.6127e-01],\n          ...,\n          [-2.5364e-01, -2.2632e-01, -1.9900e-01,  ...,  1.9821e-01,\n            1.7559e-04, -1.9786e-01],\n          [-2.1278e-01, -2.1582e-01, -2.1886e-01,  ...,  2.4907e-01,\n            9.4937e-02, -5.9194e-02],\n          [-1.7192e-01, -2.0532e-01, -2.3871e-01,  ...,  2.9993e-01,\n            1.8970e-01,  7.9471e-02]],\n\n         [[-1.5216e-02, -2.4193e-01, -4.6865e-01,  ..., -4.5016e-01,\n           -4.6535e-01, -4.8054e-01],\n          [ 7.5350e-02, -1.6612e-01, -4.0759e-01,  ..., -4.7639e-01,\n           -4.7045e-01, -4.6450e-01],\n          [ 1.6592e-01, -9.0311e-02, -3.4654e-01,  ..., -5.0263e-01,\n           -4.7554e-01, -4.4846e-01],\n          ...,\n          [-6.6184e-02,  5.2277e-02,  1.7074e-01,  ...,  7.2636e-01,\n            5.8582e-01,  4.4527e-01],\n          [ 9.4293e-03,  8.5529e-02,  1.6163e-01,  ...,  6.6322e-01,\n            5.2914e-01,  3.9507e-01],\n          [ 8.5043e-02,  1.1878e-01,  1.5252e-01,  ...,  6.0007e-01,\n            4.7247e-01,  3.4487e-01]],\n\n         [[ 1.7386e-01, -6.0707e-03, -1.8600e-01,  ...,  5.8853e-01,\n            4.8208e-01,  3.7563e-01],\n          [ 1.9302e-01,  1.7823e-02, -1.5737e-01,  ...,  6.2078e-01,\n            5.4137e-01,  4.6196e-01],\n          [ 2.1218e-01,  4.1716e-02, -1.2875e-01,  ...,  6.5302e-01,\n            6.0066e-01,  5.4830e-01],\n          ...,\n          [ 5.6445e-01,  5.5103e-01,  5.3761e-01,  ...,  5.4914e-01,\n            6.4380e-01,  7.3845e-01],\n          [ 3.5169e-01,  3.7650e-01,  4.0131e-01,  ...,  7.2203e-01,\n            7.3791e-01,  7.5379e-01],\n          [ 1.3893e-01,  2.0197e-01,  2.6501e-01,  ...,  8.9491e-01,\n            8.3202e-01,  7.6912e-01]],\n\n         ...,\n\n         [[ 2.3236e-01,  2.1675e-01,  2.0115e-01,  ...,  5.3527e-02,\n            1.5288e-01,  2.5223e-01],\n          [ 1.2893e-01,  1.7019e-01,  2.1144e-01,  ...,  2.4656e-01,\n            3.0438e-01,  3.6221e-01],\n          [ 2.5505e-02,  1.2362e-01,  2.2174e-01,  ...,  4.3960e-01,\n            4.5589e-01,  4.7219e-01],\n          ...,\n          [-3.8914e-01, -3.6045e-01, -3.3175e-01,  ...,  1.0475e-01,\n           -1.6456e-01, -4.3386e-01],\n          [-5.0501e-01, -5.0199e-01, -4.9898e-01,  ...,  4.6475e-02,\n           -2.9115e-01, -6.2877e-01],\n          [-6.2087e-01, -6.4354e-01, -6.6621e-01,  ..., -1.1798e-02,\n           -4.1774e-01, -8.2369e-01]],\n\n         [[-4.9311e-01, -4.0334e-01, -3.1357e-01,  ...,  9.5701e-01,\n            7.5951e-01,  5.6200e-01],\n          [-4.7322e-01, -3.7409e-01, -2.7496e-01,  ...,  9.3502e-01,\n            7.1602e-01,  4.9702e-01],\n          [-4.5333e-01, -3.4484e-01, -2.3635e-01,  ...,  9.1302e-01,\n            6.7254e-01,  4.3205e-01],\n          ...,\n          [ 8.6138e-02,  1.5683e-01,  2.2751e-01,  ...,  6.4080e-01,\n            5.4217e-01,  4.4354e-01],\n          [-1.4482e-02,  4.7455e-02,  1.0939e-01,  ...,  6.7146e-01,\n            5.4670e-01,  4.2193e-01],\n          [-1.1510e-01, -6.1915e-02, -8.7279e-03,  ...,  7.0211e-01,\n            5.5122e-01,  4.0033e-01]],\n\n         [[ 5.5169e-01,  6.3094e-01,  7.1018e-01,  ...,  8.3572e-01,\n            6.8007e-01,  5.2441e-01],\n          [ 5.0985e-01,  5.9613e-01,  6.8241e-01,  ...,  6.5284e-01,\n            5.0077e-01,  3.4870e-01],\n          [ 4.6801e-01,  5.6133e-01,  6.5465e-01,  ...,  4.6996e-01,\n            3.2148e-01,  1.7300e-01],\n          ...,\n          [ 1.8853e-01,  5.2261e-02, -8.4012e-02,  ...,  4.2269e-02,\n            1.2161e-01,  2.0096e-01],\n          [ 2.7119e-01,  1.1079e-01, -4.9609e-02,  ..., -5.0716e-02,\n            3.9875e-02,  1.3047e-01],\n          [ 3.5384e-01,  1.6932e-01, -1.5206e-02,  ..., -1.4370e-01,\n           -4.1864e-02,  5.9973e-02]]],\n\n\n        [[[ 1.4198e-01,  4.6102e-02, -4.9772e-02,  ...,  1.8982e-01,\n            1.1459e-01,  3.9354e-02],\n          [ 1.0650e-01,  7.8110e-03, -9.0880e-02,  ...,  1.7651e-01,\n            1.1786e-01,  5.9221e-02],\n          [ 7.1027e-02, -3.0480e-02, -1.3199e-01,  ...,  1.6319e-01,\n            1.2114e-01,  7.9087e-02],\n          ...,\n          [ 9.1231e-03,  8.8286e-02,  1.6745e-01,  ...,  5.3169e-01,\n            3.7349e-01,  2.1529e-01],\n          [-3.0396e-02,  5.2787e-02,  1.3597e-01,  ...,  3.4981e-01,\n            2.1417e-01,  7.8519e-02],\n          [-6.9916e-02,  1.7288e-02,  1.0449e-01,  ...,  1.6793e-01,\n            5.4840e-02, -5.8251e-02]],\n\n         [[-4.1140e-01, -4.7716e-01, -5.4291e-01,  ..., -4.3422e-01,\n           -2.7278e-01, -1.1133e-01],\n          [-2.3406e-01, -3.4771e-01, -4.6135e-01,  ..., -4.1526e-01,\n           -3.0350e-01, -1.9175e-01],\n          [-5.6726e-02, -2.1826e-01, -3.7979e-01,  ..., -3.9630e-01,\n           -3.3423e-01, -2.7216e-01],\n          ...,\n          [-6.2389e-02, -6.3711e-02, -6.5032e-02,  ...,  8.2687e-01,\n            7.7805e-01,  7.2924e-01],\n          [-6.7622e-02, -7.7610e-02, -8.7598e-02,  ...,  8.1005e-01,\n            7.0553e-01,  6.0101e-01],\n          [-7.2856e-02, -9.1510e-02, -1.1016e-01,  ...,  7.9324e-01,\n            6.3300e-01,  4.7277e-01]],\n\n         [[ 2.9341e-01,  3.2092e-01,  3.4842e-01,  ...,  2.2999e-03,\n           -1.1564e-01, -2.3357e-01],\n          [ 3.7647e-01,  3.6573e-01,  3.5499e-01,  ...,  1.1930e-01,\n            3.7487e-02, -4.4328e-02],\n          [ 4.5953e-01,  4.1055e-01,  3.6156e-01,  ...,  2.3631e-01,\n            1.9061e-01,  1.4492e-01],\n          ...,\n          [ 4.0038e-01,  4.1463e-01,  4.2888e-01,  ...,  1.1781e+00,\n            1.1516e+00,  1.1250e+00],\n          [ 3.5846e-01,  3.7557e-01,  3.9268e-01,  ...,  1.1848e+00,\n            1.0001e+00,  8.1540e-01],\n          [ 3.1655e-01,  3.3652e-01,  3.5648e-01,  ...,  1.1916e+00,\n            8.4867e-01,  5.0579e-01]],\n\n         ...,\n\n         [[ 2.7494e-01,  2.7871e-01,  2.8248e-01,  ...,  2.4434e-01,\n            1.3079e-01,  1.7235e-02],\n          [ 2.2062e-01,  2.8941e-01,  3.5819e-01,  ...,  2.4628e-01,\n            1.1970e-01, -6.8836e-03],\n          [ 1.6631e-01,  3.0010e-01,  4.3389e-01,  ...,  2.4822e-01,\n            1.0861e-01, -3.1002e-02],\n          ...,\n          [-4.9930e-01, -3.5033e-01, -2.0136e-01,  ...,  2.7132e-01,\n            1.4350e-01,  1.5675e-02],\n          [-5.3220e-01, -4.2772e-01, -3.2324e-01,  ...,  3.0964e-01,\n            1.4534e-01, -1.8950e-02],\n          [-5.6510e-01, -5.0511e-01, -4.4512e-01,  ...,  3.4796e-01,\n            1.4719e-01, -5.3576e-02]],\n\n         [[ 5.7772e-02,  6.5279e-02,  7.2785e-02,  ...,  2.4636e-01,\n            2.9675e-02, -1.8701e-01],\n          [ 2.4406e-03,  5.2156e-02,  1.0187e-01,  ...,  1.5547e-01,\n           -4.1076e-02, -2.3762e-01],\n          [-5.2890e-02,  3.9034e-02,  1.3096e-01,  ...,  6.4575e-02,\n           -1.1183e-01, -2.8823e-01],\n          ...,\n          [-2.5416e-01, -1.8782e-01, -1.2147e-01,  ...,  6.8336e-01,\n            5.7187e-01,  4.6039e-01],\n          [-2.7431e-01, -2.1432e-01, -1.5434e-01,  ...,  5.8468e-01,\n            4.7151e-01,  3.5834e-01],\n          [-2.9447e-01, -2.4083e-01, -1.8720e-01,  ...,  4.8599e-01,\n            3.7114e-01,  2.5629e-01]],\n\n         [[ 4.7829e-01,  4.5462e-01,  4.3094e-01,  ...,  4.9921e-01,\n            3.1630e-01,  1.3339e-01],\n          [ 3.7468e-01,  3.3614e-01,  2.9760e-01,  ...,  3.8753e-01,\n            2.2490e-01,  6.2264e-02],\n          [ 2.7108e-01,  2.1767e-01,  1.6425e-01,  ...,  2.7585e-01,\n            1.3349e-01, -8.8659e-03],\n          ...,\n          [ 5.3552e-01,  5.1002e-01,  4.8453e-01,  ..., -7.7814e-01,\n           -6.7721e-01, -5.7627e-01],\n          [ 4.5427e-01,  4.4918e-01,  4.4410e-01,  ..., -7.8969e-01,\n           -6.5283e-01, -5.1596e-01],\n          [ 3.7301e-01,  3.8834e-01,  4.0366e-01,  ..., -8.0123e-01,\n           -6.2844e-01, -4.5565e-01]]]], device='cuda:0',\n       grad_fn=<UpsampleBilinear2DBackward1>)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "C,H,W = INPUTSIZE\n",
    "model(torch.randn(BATCH,C,H,W).to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58c22139e035fe019fb6d4ad64076bc2c6d0447a07f25eee7aec180e2958d9c4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('alchera': conda)",
   "metadata": {
    "interpreter": {
     "hash": "db56930f87fd6ced40042838fcad864acfd5dacca559a306e759b385ddadcd99"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}